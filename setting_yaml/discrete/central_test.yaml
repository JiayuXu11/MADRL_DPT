algorithm_name: 'happo' #'happo' #'heuristic2'  #'happo_continue'
experiment_name: 'central_controller'

# basic setting
model_dir:  "C:/Users/Jerry/Desktop/thesis/code/MADRL/MADRL_main/setting_yaml/error_model/central_controller"
n_rollout_threads: 300
num_env_steps: 300000000
num_agents: 3
eval_interval: 20
n_no_improvement_thres: 30
gamma: 0.98

# continue or discrete
action_type: 'discrete'  # ['discrete', 'multi_discrete', 'continue']

# env
alpha: 0.7
ratio_transship: False
transship_revenue_method: 'ratio'
constant_transship_revenue: 0.1
ratio_transship_revenue: 0.7

central_controller: True
obs_transship: 'all_transship'
actor_obs_step: False
train_episode_length: 195

# fluctuation control
entropy_decrease: False
entropy_decrease_list: [0.5,0.1,0.01,0.]
entropy_coef: 0.01
std_y_coef: [10.0,10.0]  # only for continue
std_x_coef: 1.0


# lr
use_linear_lr_decay: False
lr: 5e-4
critic_lr: 1e-3

# model setting
recurrent_N: 2
layer_N: 2
hidden_size: 256
hidden_size_critic: 64 # critic
use_ReLU: True
weight_decay: 0

# ppo
ppo_epoch: 10
use_proper_time_limits: False
use_gae: True
gae_lambda: 0.95
use_clipped_value_loss: True
clip_param: 0.2
critic_learning_pure_returns: True

# normalization
sample_mean_advantage: True
use_valuenorm: True
use_popart: False
use_feature_normalization: False
use_max_grad_norm: True
max_grad_norm: 0.5


instant_info_sharing: True
central_controller: True
advantage_pure_returns: False


