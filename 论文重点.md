## 问题设定
1. decentral proactive transship的**复杂性**，难以用传统方法解决
2. 实际应用场景中，**中心化决策可行性较弱**。加盟店很多，中心化决策难以实施，且即便在直营店的背景下，中心化决策也是相当具有挑战性的。
3. 在happo中，如是体现decentral：**局部信息+自私目标**。局部信息加强了实际场景中的可行性，自私目标使得零售商更愿意将之实施。鲁棒性：实验表明，即便在部分零售商不听取建议的情况下，仍有较好的结果。
4. transship的成交机制
## 强化学习
1. 不论是transship问题，还是drl在供应链问题的应用，多聚焦在决策单一行动，而非**联合决策**。
2. 没有使用alpha，因其一定程度破坏了decentral的特性，**但可通过调整transship价格，达到了类似的效果**。（总公司可通过调整transship的价格，调动零售商间的transship流动，从而获得更高的总体收益）
3. **critic network的输入加入了未来需求数据**的相关信息（quantile、mean、std），听起来不合理，但其实是可行的（CTDE），且能有效提升训练效果。
4. 在pure_returns下，return的波动范围较大（step=0与step=199），并不利于critic network学习。而常规的标准化形式也不能有效解决该问题。因而选择**对cost进行了based on demand的标准化**，有效提升训练效果。
5. 使用较大n_rollout_threads能提升训练稳定性；decreasing entropy
##  实验比对与管理启示
1. **灵敏度分析：lead time, cost coefficient** 每个参数下，设置四组策略横向对比：ss+T、ss、happo+T、happo（没有transship，这里直接沿用happo+T的模型就好了，然后不执行transship）。**理想结果：** （1）有transship比没有好，且提升的数值大小与参数设定有一定规律可循 (2) happo+T效果比ss+T好
2. **信息共享。** 过往transship的传统研究，多基于已知所有零售商库存需求等信息的假设，但这些是否会较大影响transship的效果呢？**理想结果：** 信息共享可以提高分数，但效果有限。**启示：** 信息共享有效，但非必要，即便在现实场景中更为常见的无信息披露的情况下，强化学习仍有出色的效果
3. **中心化决策。** 过往transship的传统研究+强化学习的几篇文章都是本质上都是中心化决策。即便是happo那篇，其内部驱动也是最小化总体收益。**理想结果：** single agent network表现不如调整alpha的multi agent(1)，也不如decentral的情况(2)。decentral不如调整了alpha的multi agent，但相差不多(3)。**启示：** (1)多智能体强化学习的literature表明，虽然表现最优的multi agent的目标函数并不是总体利益，但训练结果往往能达到总体利益最优（指和其他训练结果比），因而相比于single agent network，更能代表中心化决策的效果。(2) 没什么可说的 (3) decentral 能达到central的xx%, 即便在现实场景中更为常见的decentral的情况下，强化学习仍有出色的效果
4. **策略非完全执行的鲁棒性。** 在实际场景中，强制要求所有零售商按照强化学习策略去执行，是颇具挑战性的。因而我们需探明，当部分零售商不参照强化学习策略采取行动，或部分时间内不参照强化学习策略采取行动时，强化学习是否仍会有不错的表现。**理想结果：** 非完全执行下，仍有xx%的效果。**启示：** 策略非完全执行下，表现仍不错，具有鲁棒性。
5. **实际数据集上的应用。** **理想结果：** 比ss+T好。**启示：** 在实际应用场景下可以落地
6. 处在中心地区的零售商，因为交通便利，可能会倾向于多订货，以满足周边transship需求