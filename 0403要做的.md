1. gae return的时间问题  问题不大
2. transship信息不一定要
3. multi_discrete
4. lead time要可调节，那随之而来的可能就是action dim的增大  已完成
5. 数据波动性加大
6. central controller必须要去做了


后面的主要工作可能要在神经网络模型上了：
1. multi-discrete 已完成
2. central controller 已完成
3. mqrnn
4. critic network的需求输入加上未来需求quantile
5. 变成penalty+holding，不追求正数了。因为感觉加上卖货赚的钱，那个critic network不太好预测，因为多了一部分只和需求相关的部分  已完成
6. 换一个数据生成过程吧
7. 试一下杉树那个数据
8. 当前的transship机制，是基于transship cost一致的基础上的，若不一致则会更复杂
9. 让central的transship就是实际的transship
10. central效果还是不好，要debug检查一下，看action是不是都对齐了
11. reward normalization

## 工作总结
1. 完成了multi-discrete（可以接受较大的action空间）的编写与运行，但效果不如discrete
2. central controller的bug找到了，跑出来的分数正常了（就是高于5分了，之前是1分以下），但效果仍不如decentral的
3. 尝试将目标最大化reward变成了最小化cost，虽然两个问题是等价的，但训练结果普遍更差了（5.703<5.792）。猜测是由于reward的数值绝对值比cost的绝对值小，导致训练上更加容易。目前正对reward进行进一步标准化，以加强这一效果。
4. 信息共享下，模型效果有所提升（5.816>5.792）

## 待做
1. 对reward进行标准化(弄好了，完全弄好了)
2. 参考其他强化学习项目，修改multi-discrete与central controller 的网络模型
3. 基于当下内容对论文结构进行梳理
4. transship撮合机制参考ms那篇文章（Huffman Tree），优先最近的，这样就可以考虑不同地区transship的运费不同的情况（没太理解那篇文章为啥要搞个树，直接距离从近到远弄不就行了吗？）
5. 一些指标加入tensorboard里（加完了）
6. advantage用的gae（check过了）
7. multi_quantile demand(我之前好像设置过只披露未来1/(1-gamma)的所有信息，但效果不好，具体要再check一下可能)
8. 如果只训order，或者transship呢？或者这两个分开训呢？

## 调参必读
1. critic_pure_returns 与 advantage_pure_returns 必须同时设为True/False