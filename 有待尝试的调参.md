1. use_relu 调为 False，默认就是tanh
2. state normalization
3. 1/(1-γ)用来估计agent做决策时往前考虑的步数，0.95-20, 0.90-10,0.85-7,0.8-5
4. reward shaping使得actor不要一次才订5以下的货，然后鼓励需求少的agent通过transship订货，快到200天就别订货了,或者通过此让他学习baseline的订货
5. tensorboard看看随着训练entropy变化情况
6. 模仿学习
7. pop-art
8. action masking，没有那么多货就别让它想着transship那么多货出去
9. 有界beta分布
10. eval一起扔进去，不要一个一个跑
11. 看看他们玩星际，网络是怎么设计的，星际action空间这么大
12. 两个波动性应该统一，因为学习率是统一设置的
13. actor网络因为要输出比较大的向量，可能要设置复杂一点；但critic应该不用吧，但现在二者网络模型是完全绑定的
    