1. use_relu 调为 False，默认就是tanh
2. state normalization
3. 1/(1-γ)用来估计agent做决策时往前考虑的步数，0.95-20, 0.90-10,0.85-7,0.8-5
4. reward shaping使得actor不要一次才订5以下的货，然后鼓励需求少的agent通过transship订货，快到200天就别订货了,或者通过此让他学习baseline的订货
5. tensorboard看看随着训练entropy变化情况
6. 模仿学习
7. pop-art
8. action masking，没有那么多货就别让它想着transship那么多货出去
9. 有界beta分布
10. eval一起扔进去，不要一个一个跑
11. 看看他们玩星际，网络是怎么设计的，星际action空间这么大
12. 两个波动性应该统一，因为学习率是统一设置的
13. actor网络因为要输出比较大的向量，可能要设置复杂一点；但critic应该不用吧，但现在二者网络模型是完全绑定的
14. lr_decay
15. 递进调整entropy loss， 这个的本意是让模型不要那么快收敛，从而探索更多可能性。但调到0.1还是很快收敛，而且后面降到0.01以下,训练的时候也不怎么动了，
16. weight decay=5e-4?
17. 网络设计上，actor先对不同类别信息分别处理，然后concat起来，然后过两遍lstm，再分别输出不同action；critic需要用rnn吗？感觉没必要,着重看一下
18. active masks 一般来说好像没有要把自己的或全部捐出去的行为，所以可能改了效果也不明显
19. transship的revenue也开个口子
20. 基于Pseudo-reward的思想， 如果某agent缺货了，其他agent也要承受一定的penalty cost，如果该agent没有找他们要，更要惩罚。如果当天明明缺货了还要给别人货的惩罚，或者因为要给别人货而缺货的，惩罚。
    